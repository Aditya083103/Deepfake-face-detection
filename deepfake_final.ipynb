{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8f20WYD4twL",
        "outputId": "8b63e341-c3cd-4734-d137-36cd2dc69ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split_folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZfvTIP56AD",
        "outputId": "6ec9f3f3-d4c6-47b0-af86-0c4371a55f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split_folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split_folders\n",
            "Successfully installed split_folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras_applications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H0OaGD-9V2r",
        "outputId": "38cfb825-7012-4311-df17-e0f530fe8739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.11.0)\n",
            "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_applications\n",
            "Successfully installed keras_applications-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from distutils.dir_util import copy_tree\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "print('TensorFlow version: ', tf.__version__)\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/deepfake project final/DeepFake-Detect/split_dataset'\n",
        "\n",
        "tmp_debug_path = './tmp_debug'\n",
        "print('Creating Directory: ' + tmp_debug_path)\n",
        "os.makedirs(tmp_debug_path, exist_ok=True)\n",
        "\n",
        "def get_filename_only(file_path):\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    filename_only = file_basename.split('.')[0]\n",
        "    return filename_only\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "input_size = 128\n",
        "batch_size_num = 16\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "val_path = os.path.join(dataset_path, 'val')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255,    # Rescale the tensor values to [0,1]\n",
        "    rotation_range = 10,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_path,\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=batch_size_num,\n",
        "    class_mode='binary',  # Binary classification\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255    # Rescale the tensor values to [0,1]\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    directory=val_path,\n",
        "    target_size=(input_size, input_size),\n",
        "    batch_size=batch_size_num,\n",
        "    class_mode='binary',  # Binary classification\n",
        "    shuffle=False  # Disable shuffling\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255    # Rescale the tensor values to [0,1]\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_path,\n",
        "    classes=['real', 'fake'],\n",
        "    target_size=(input_size, input_size),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=None,\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Train a CNN classifier\n",
        "efficient_net = EfficientNetB0(\n",
        "    weights='imagenet',\n",
        "    input_shape=(input_size, input_size, 3),\n",
        "    include_top=False,\n",
        "    pooling='max'\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(efficient_net)\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = './tmp_checkpoint'\n",
        "print('Creating Directory: ' + checkpoint_filepath)\n",
        "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
        "\n",
        "custom_callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=os.path.join(checkpoint_filepath, 'best_model.keras'),  # Save as .keras file\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=1,\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train network\n",
        "num_epochs = 8\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=len(train_generator),  # Ensure steps_per_epoch is correctly defined\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_generator),  # Ensure validation_steps is set to match the size of val_generator\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=2  # Verbose logging to track the training process\n",
        ")\n",
        "\n",
        "print(history.history)\n",
        "\n",
        "# Load the saved model that is considered the best\n",
        "best_model = load_model(os.path.join(checkpoint_filepath, 'best_model.keras'))  # Ensure consistent loading\n",
        "\n",
        "# Generate predictions\n",
        "test_generator.reset()\n",
        "\n",
        "preds = best_model.predict(\n",
        "    test_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Flatten predictions for easier handling\n",
        "test_results = pd.DataFrame({\n",
        "    \"Filename\": test_generator.filenames,\n",
        "    \"Prediction\": preds.flatten()\n",
        "})\n",
        "\n",
        "print(test_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "L9ODuu-T5RUD",
        "outputId": "2e58fbd2-8a03-4458-8596-524a6e3371e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:  2.17.0\n",
            "Creating Directory: ./tmp_debug\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n",
            "Creating Directory: ./tmp_checkpoint\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7fe78cfcbac0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.70354, saving model to ./tmp_checkpoint/best_model.keras\n",
            "1/1 - 59s - 59s/step - accuracy: 0.3750 - loss: 1.9758 - val_accuracy: 0.5000 - val_loss: 0.7035\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'items'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-900ff7adec50>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Train network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 )\n\u001b[1;32m    353\u001b[0m                 val_logs = {\n\u001b[0;32m--> 354\u001b[0;31m                     \u001b[0;34m\"val_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m                 }\n\u001b[1;32m    356\u001b[0m                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
          ]
        }
      ]
    }
  ]
}